{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only permit vertex cover\n",
    "\n",
    "canonical_order = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz',  'MUTAG', 'ENZYMES', 'PROTEINS',   'IMDB-BINARY', 'COLLAB']\n",
    "def reorder(df, canonical_order=canonical_order, by='dataset', extras=['dataset','gen_n', 'gen_n_max'], secondary='gen_n', columns=None):\n",
    "    df['dataset_name_order'] = df[by].map({name: i for i, name in enumerate(canonical_order)})\n",
    "    if secondary is not None:\n",
    "        df = df.sort_values(by=['dataset_name_order', secondary])\n",
    "    else:\n",
    "        df = df.sort_values(by=['dataset_name_order'])\n",
    "\n",
    "    df.drop('dataset_name_order', axis =1, inplace=True)\n",
    "\n",
    "    if columns:\n",
    "        return df[columns ]\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct gurobi 8.0 score baseline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_folder = Path('/home/bcjexu/maxcut-80/bespoke-gnn4do/')\n",
    "sys.path.insert(0, str(root_folder))\n",
    "\n",
    "from utils.tabulate import load_baseline_outputs\n",
    "\n",
    "baseline_folders = ['baseline_runs/230928_gurobi']\n",
    "\n",
    "gurobi_8_dict = {}\n",
    "\n",
    "for baseline_folder in baseline_folders:\n",
    "\n",
    "    for model_folder in os.listdir(os.path.join(root_folder, baseline_folder)):\n",
    "        with open(os.path.join(os.path.join(root_folder, baseline_folder, model_folder), 'params.txt'), 'r') as f:\n",
    "            model_args = json.load(f)\n",
    "        if model_args['problem_type'] == 'vertex_cover':\n",
    "            continue\n",
    "        \n",
    "        #print(model_args['gurobi'], model_args['gurobi_timeout'], model_args['dataset'], model_args['gen_n'])\n",
    "        #print(load_baseline_outputs(Path(os.path.join(root_folder, baseline_folder)), model_folder, 'gurobi', indices))\n",
    "\n",
    "        if model_args[\"gurobi_timeout\"] != 8:\n",
    "            continue\n",
    "        row = f'gurobi_{model_args[\"gurobi_timeout\"]}'\n",
    "        if isinstance(model_args['gen_n'], list):\n",
    "            col = f\"{model_args['dataset']}@@{model_args['gen_n'][0]}\"\n",
    "        else:\n",
    "            col = f\"{model_args['dataset']}\"\n",
    "        #print(row,col, Path(os.path.join(root_folder, baseline_folder)), model_folder)\n",
    "\n",
    "        scores = []\n",
    "        with open(Path(os.path.join(root_folder, baseline_folder, model_folder))  / 'results.jsonl', 'r') as f:\n",
    "\n",
    "            for line in f:\n",
    "                res = json.loads(line)\n",
    "                # second condition is: only do this if the graph is in the validation set\n",
    "                #print(\"A\")\n",
    "                #assert(indices == None or dataset not in indices or res['index'] in indices[dataset])\n",
    "                #print(\"B\")\n",
    "                scores.append(res['score'])\n",
    "        #print(scores)\n",
    "        gen_n = tuple(model_args['gen_n']) if isinstance(model_args['gen_n'], list) else \"\"\n",
    "        gurobi_8_dict[(model_args['dataset'], gen_n)] = scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legacy',\n",
       " 'Testing',\n",
       " '230928_snapshot',\n",
       " 'LiftMP_runs',\n",
       " '230924_hparam2',\n",
       " '230927_snapshot',\n",
       " '230928_runs',\n",
       " '230924_hparam',\n",
       " '230924_hparam_TU_multiarch',\n",
       " '230924_hparam_TU',\n",
       " '230926_finetune_ER_runs']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['230927_snapshot/230925_TUsmall_GAT_VC', '230927_snapshot/230925_TUsmall_GIN_VC', '230927_snapshot/230925_generated_preset_cut', '230927_snapshot/230925_TUlarge_all_cut', '230927_snapshot/230925_TUsmall_GCNN_VC', '230927_snapshot/230925_generated_liftMP_VC', '230927_snapshot/230925_TUlarge_all_VC', '230927_snapshot/230925_generated_preset_VC', '230927_snapshot/230925_TUsmall_liftMP_VC', '230927_snapshot/230925_TUsmall_GatedGCNN_VC', '230927_snapshot/230925_generated_liftMP_cut']\n"
     ]
    }
   ],
   "source": [
    "print(list(os.path.join('230927_snapshot', x) for x in os.listdir('/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs/230927_snapshot')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_folders = ['LiftMP_runs', '230927_snapshot/230925_TUsmall_GAT_VC', '230927_snapshot/230925_TUsmall_GIN_VC', \n",
    "               '230927_snapshot/230925_generated_preset_cut', '230927_snapshot/230925_TUlarge_all_cut', '230927_snapshot/230925_TUsmall_GCNN_VC',\n",
    "                 '230927_snapshot/230925_generated_liftMP_VC', '230927_snapshot/230925_TUlarge_all_VC', '230927_snapshot/230925_generated_preset_VC', \n",
    "                 '230927_snapshot/230925_TUsmall_liftMP_VC', '230927_snapshot/230925_TUsmall_GatedGCNN_VC', '230927_snapshot/230925_generated_liftMP_cut']\n",
    "\n",
    "run_folders = ['230928_snapshot/230925_TUsmall_GAT_VC', '230928_snapshot/230925_TUsmall_GIN_cut', \n",
    "               '230928_snapshot/230925_generated_preset_cut', '230928_snapshot/230925_TUsmall_GAT_cut', \n",
    "               '230928_snapshot/230925_TUsmall_liftMP_cut', '230928_snapshot/230925_TUsmall_GCNN_VC', \n",
    "               '230928_snapshot/230925_TUsmall_GCNN_cut', '230928_snapshot/230925_generated_liftMP_VC', \n",
    "               '230928_snapshot/230925_generated_preset_VC', '230928_snapshot/230925_TUsmall_liftMP_VC', '230928_snapshot/230925_TUsmall_GatedGCNN_VC', \n",
    "               '230928_snapshot/230925_TUsmall_VC_32', '230928_snapshot/230925_generated_liftMP_cut', '230928_snapshot/230925_TUsmall_GatedGCNN_cut']\n",
    "\n",
    "run_folders = ['230928_runs/230925_TUsmall_GAT_VC', '230928_runs/230925_TUsmall_GIN_cut', \n",
    "               '230928_runs/230925_generated_preset_cut', '230928_runs/230925_TUsmall_GAT_cut', \n",
    "               '230928_runs/230925_TUsmall_liftMP_cut', '230928_runs/230925_TUsmall_GCNN_VC', \n",
    "               '230928_runs/230925_TUsmall_GCNN_cut', '230928_runs/230925_generated_liftMP_VC', '230928_runs/230925_generated_preset_VC', \n",
    "               '230928_runs/230925_TUsmall_liftMP_VC', '230928_runs/230925_TUsmall_GatedGCNN_VC', '230928_runs/230925_TUsmall_VC_32', \n",
    "               '230928_runs/230925_TUlarge_liftMP_cut', '230928_runs/230925_TUlarge_liftMP_VC', '230928_runs/230925_generated_liftMP_cut', '230928_runs/230925_TUsmall_GatedGCNN_cut']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs'\n",
    "model_list = [os.path.join(folder_path, run_folder, x) for run_folder in run_folders for x in os.listdir(os.path.join(folder_path, run_folder))  ]\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "errored = []\n",
    "numtimes = 0\n",
    "for model_folder in model_list:\n",
    "    try:\n",
    "        with open(os.path.join(model_folder, 'params.txt'), 'r') as f:\n",
    "            model_args = json.load(f)\n",
    "        if model_args['problem_type'] == 'vertex_cover':\n",
    "            continue\n",
    "        \n",
    "        losses = np.load(os.path.join(model_folder, 'valid_scores.npy'))\n",
    "        test_losses = np.load(os.path.join(model_folder, 'test_scores.npy'))\n",
    "\n",
    "        modeldict = model_args #{x: model_args[x] for x in params}\n",
    "        modeldict['max_valid_score'] = max(losses)\n",
    "        modeldict['max_valid_epoch'] = np.argmax(losses)\n",
    "        #modeldict['scores'] = test_losses[np.argmax(losses)]\n",
    "        modeldict['baseline'] = False\n",
    "\n",
    "        # reset test score if the validation is better.\n",
    "        valid_score_from_file = -np.inf\n",
    "        test_score_from_file = -np.inf\n",
    "        test_std_from_file = np.nan\n",
    "\n",
    "        for prefix in ['revalidate_best', 'revalidate_last']:\n",
    "            scorefile = [x for x in os.listdir(model_folder) if x.startswith(prefix)]\n",
    "            #assert(len(scorefile) <=1)\n",
    "            if len(scorefile) >= 1:\n",
    "                _, scores = np.load(os.path.join(model_folder, scorefile[0]))\n",
    "                valid_score = np.average(scores)\n",
    "\n",
    "                if valid_score > valid_score_from_file:\n",
    "                    testscorefile = [x for x in os.listdir(model_folder) if x.startswith(prefix.replace('validate', 'test'))]\n",
    "                    _, test_scores =  np.load(os.path.join(model_folder, testscorefile[0]))\n",
    "                    # get gurobi\n",
    "                    gen_n = tuple(model_args['gen_n']) if isinstance(model_args['gen_n'], list) else \"\"\n",
    "                    gurobi_scores = gurobi_8_dict[(model_args['dataset'], gen_n)]\n",
    "                    norms = [x/y for x, y in zip(test_scores, gurobi_scores)]\n",
    "                    # set score\n",
    "                    test_score_from_file = np.average(norms)\n",
    "                    test_std_from_file = np.std(norms)\n",
    "                    valid_score_from_file = valid_score\n",
    "        \n",
    "        if valid_score_from_file > modeldict['max_valid_score']:\n",
    "            modeldict['max_valid_score'] = valid_score_from_file\n",
    "            modeldict['scores'] = test_score_from_file\n",
    "            modeldict['stdev'] = test_std_from_file\n",
    "        else:\n",
    "            scorefile = [x for x in os.listdir(model_folder) if x.startswith(\"retest_best\")]\n",
    "            #assert(len(scorefile) <=1)\n",
    "            if len(scorefile) >= 1:\n",
    "                times, scores = np.load(os.path.join(model_folder, scorefile[0]))\n",
    "                #modeldict['stdev'] = np.std(scores)\n",
    "                modeldict['full_scores'] = scores\n",
    "                numtimes+=1\n",
    "                gen_n = tuple(model_args['gen_n']) if isinstance(model_args['gen_n'], list) else \"\"\n",
    "                gurobi_scores = gurobi_8_dict[(model_args['dataset'], gen_n)]\n",
    "                \n",
    "                norms = [x/y for x, y in zip(scores, gurobi_scores)]\n",
    "                #print(model_args['dataset'])\n",
    "                #print(list(zip(scores, gurobi_scores)))\n",
    "                #print(np.average(scores)/np.average(gurobi_scores), norms)\n",
    "                modeldict['scores'] = np.average(norms)\n",
    "                #print(norms)\n",
    "                modeldict['stdev'] = np.std(norms)\n",
    "                #print(len(scores), len(gurobi_scores))\n",
    "            \n",
    "            else: \n",
    "                # load a dummy\n",
    "                #times, scores = np.load('/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs/230924_hparam/paramhash:0a0656a369a5b8e4a4be27e0d04fb3b8c161e7b630caf99b8eaeedcddd6a2b18/time_and_score@@test_results_2023-09-28_01:23:33.np.npy')\n",
    "                modeldict['scores'] = np.nan\n",
    "                modeldict['stdev'] = np.nan\n",
    "        \n",
    "\n",
    "        rows.append(modeldict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'{e} is wrong w/ {model_folder}')\n",
    "        errored.append(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in baselines\n",
    "baselines = pd.read_csv('vc_baseline_scores.csv')\n",
    "\n",
    "gen_n_dict = dict(zip([50, 100, 400], [[50, 100], [100, 200],[400, 500]]))\n",
    "\n",
    "# unwind them \n",
    "for i, baseline in baselines.iterrows():\n",
    "    #print(baseline.index)\n",
    "    for col in baselines.keys():\n",
    "        if col == 'Unnamed: 0':\n",
    "            continue\n",
    "        ds = col\n",
    "        gen_n = np.nan\n",
    "        if len(col.split('@@')) > 1:\n",
    "            ds, gen_n = col.split('@@')\n",
    "            gen_n = gen_n_dict[int(gen_n)]\n",
    "\n",
    "        row = {'dataset': ds, 'gen_n': gen_n, 'model_type': baseline['Unnamed: 0'], 'scores': baseline[col], 'baseline': True}\n",
    "        #print(row)\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "Counter(df[df.gen_n == 400].model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in df.gen_n:\n",
    "    if not isinstance(x, list) and x !=100:\n",
    "        print(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gen_n_list'] = df.gen_n\n",
    "df['gen_n_max'] = df.gen_n_list.apply(lambda x: int(x[1]) if isinstance(x,list) else x)\n",
    "df.gen_n = df.gen_n.apply(lambda x: int(x[0]) if isinstance(x,list) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df.gen_n_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x, y in zip(df.gen_n, df.gen_n_max):\n",
    "    if x!=y : print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()\n",
    "# 'problem_type', 'seed',  'prefix', 'RB_n', 'RB_k', 'log_dir',\n",
    "relevant_keys = [ 'model_type', 'num_layers',\n",
    "       'repeat_lift_layers', 'num_layers_project', 'rank', 'vc_penalty', 'gen_n', 'gen_n_max',\n",
    "       'dataset', 'infinite',  'positional_encoding', 'pe_dimension',\n",
    "       'max_valid_score', 'max_valid_epoch',\n",
    "       'scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gen_n = df.gen_n.fillna(\"\")\n",
    "df.gen_n_max = df.gen_n_max.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test models\n",
    "\n",
    "models_for_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_arc = pd.DataFrame()\n",
    "dss = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz']\n",
    "mts = ['SDP proj', 'gurobi_2.0', 'gurobi_4.0', 'gurobi_8.0', 'vertex count']\n",
    "\n",
    "\n",
    "\n",
    "for (mt, ds, gen_n), group in df[(df.infinite == False) | (df.dataset == 'ErdosRenyi')].groupby(['model_type', 'dataset', 'gen_n']):\n",
    "    if mt in mts:\n",
    "        continue\n",
    "    if all(group['max_valid_score'].isna()):\n",
    "        continue\n",
    "    if ds not in dss:\n",
    "        gen_n = \"\"\n",
    "\n",
    "    #print(mt, ds, gen_n)\n",
    "    bestidx = group['max_valid_score'].idxmax()\n",
    "\n",
    "    score_writeout = f'{df.loc[bestidx][\"scores\"]:0.3f} +/- {df.loc[bestidx][\"stdev\"]:0.3f}'\n",
    "\n",
    "    if ds in dss:\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', mt] = score_writeout\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', 'gen_n'] = gen_n\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', 'dataset'] = ds\n",
    "    else: \n",
    "        dataset_by_arc.at[f'{ds}', 'gen_n'] = gen_n\n",
    "        dataset_by_arc.at[f'{ds}', mt] = score_writeout\n",
    "        dataset_by_arc.at[f'{ds}', 'dataset'] = ds\n",
    "\n",
    "    try:\n",
    "        models_for_test.append((df.loc[group['scores'].idxmax()]['log_dir'], df.loc[group['scores'].idxmax()]['dataset'], df.loc[group['scores'].idxmax()]['gen_n'])) \n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset_by_arc['dataset'] = dataset_by_arc.index\n",
    "reorder(dataset_by_arc[[k for k in dataset_by_arc.keys() if k not in ['SDP lift', 'edge count']]], by='dataset', columns = ['dataset', 'gen_n', 'GAT', \n",
    "                                                                                                                            'GCNN', 'GIN', 'GatedGCNN', 'LiftMP']).to_csv('Table1_MC_std.csv', index=False) #.style.highlight_max(color = 'green', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_arc = pd.DataFrame()\n",
    "mts = ['GAT', 'GCNN', 'GIN', 'GatedGCNN']\n",
    "dss = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz']\n",
    "\n",
    "for (mt, ds, gen_n, gen_nmax), group in df[~df.model_type.isin(mts)].groupby(['model_type', 'dataset', 'gen_n', 'gen_n_max']):\n",
    "\n",
    "    #if mt =='gurobi_4.0': print(ds) # and ds =='MUTAG': print(\"hi\")\n",
    "\n",
    "    if all(group['max_valid_score'].isna()):\n",
    "        continue\n",
    "    if ds not in dss:\n",
    "        gen_n = \"\"\n",
    "        gen_nmax = \"\"\n",
    "    #print(mt, ds, gen_n)\n",
    "    if ds in dss:\n",
    "        k = f'{ds}_{gen_n}'\n",
    "    else:\n",
    "        k = ds\n",
    "    dataset_by_arc.at[k, 'Type'] = ds\n",
    "    dataset_by_arc.at[k, 'Nmin'] = gen_n\n",
    "    dataset_by_arc.at[k, 'Nmax'] = gen_nmax\n",
    "\n",
    "    bestidx = group['max_valid_score'].idxmax()\n",
    "    score_writeout = f'{df.loc[bestidx][\"scores\"]:0.3f} +/- {df.loc[bestidx][\"stdev\"]:0.3f}'\n",
    "    \n",
    "    if mt not in  ['LiftMP'] + mts:\n",
    "        dataset_by_arc.at[k, mt] = -1*group['scores'].max()\n",
    "    else:\n",
    "        dataset_by_arc.at[k, mt] = score_writeout\n",
    "    try:\n",
    "        models_for_test.append((df.loc[group['scores'].idxmax()]['log_dir'], df.loc[group['scores'].idxmax()]['dataset'], df.loc[group['scores'].idxmax()]['gen_n'])) \n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "\n",
    "#dataset_by_arc.rename(columns={'Nikos': 'CustomLiftProject'}, inplace=True)\n",
    "reorder(dataset_by_arc[[k for k in dataset_by_arc.keys() if k not in ['SDP lift', 'vertex count']]], by='Type', secondary='Nmin').round(2).to_csv('Table2_MC_std.csv') #.style.highlight_max(color = 'green', axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxcut-80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
